{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "# Online control\n",
    "\n",
    "This notebook presents the **online control** of an agent by SARSA and Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TicTacToe, Nim, ConnectFour\n",
    "from agent import Agent, OnlineControl\n",
    "from dp import ValueIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "* You can work alone or in a team of 2.\n",
    "* Indicate your name(s) here: XXX, YYY\n",
    "* The script ``dp`` must contain your solution to ``ValueIteration`` (cf. lab on Dynamic Programming).\n",
    "* Do not import any other code / library.\n",
    "* Do not copy-paste any code or text from other students / teams.\n",
    "* Be concise in your answers.\n",
    "* Use at most 5 figures or tables.\n",
    "* Make sure that your notebook runs without errors.\n",
    "* Save your notebook with the figures / table.\n",
    "* Upload your notebook on Moodle (one per team)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the class ``SARSA`` and test it on Tic-Tac-Toe.\n",
    "* Complete the class ``QLearning`` and test it on Tic-Tac-Toe.\n",
    "* Compare these algorithms on Tic-Tac-Toe (play first) and Nim (play second), using a random adversary, then a perfect adversary. Comment your results.\n",
    "* Test these algorithms on Connect 4 against a random adversary. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect four - state space is huge, these techniques are not suitable for this game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just concise figures and tables explaining the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't need to play with the initial value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test weather a new state is terminal or not\n",
    "if it is terminal, then the reward is the reward of the game\n",
    "if it is not terminal, then the reward is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(OnlineControl):\n",
    "    \"\"\"Online control by SARSA.\"\"\"\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"Learn the state-action value online.\"\"\"\n",
    "        self.model.reinit_state()\n",
    "        state = self.model.state\n",
    "        action = self.get_best_action(state, randomized=True) \n",
    "        for t in range(self.n_steps):\n",
    "            state_code = self.model.encode(state)\n",
    "            self.state_action_count[state_code][action] += 1\n",
    "            reward, stop = self.model.step(action)\n",
    "            # to be modified\n",
    "            # begin\n",
    "            gain = 0\n",
    "            # end\n",
    "            diff = gain - self.state_action_value[state_code][action]\n",
    "            count = self.state_action_count[state_code][action]\n",
    "            self.state_action_value[state_code][action] += diff / count\n",
    "            if stop:\n",
    "                break\n",
    "            # to be modified\n",
    "            # begin\n",
    "            state = state\n",
    "            action = action\n",
    "            # end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(OnlineControl):\n",
    "    \"\"\"Online control by Q-learning.\"\"\"\n",
    "        \n",
    "    def learn(self):\n",
    "        \"\"\"Learn the state-action value online.\"\"\"\n",
    "        self.model.reinit_state()\n",
    "        state = self.model.state\n",
    "        action = self.get_best_action(state, randomized=True) \n",
    "        # to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "agent = Agent(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Control = QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = Control(game, eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 10\n",
    "for i in range(n_games):\n",
    "    algo.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(game, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(agent.get_gains(), return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
